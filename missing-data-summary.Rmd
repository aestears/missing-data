---
title: "Missing data summary"
author: "Matt T"
date:  "`r Sys.Date()`"
output:
  html_document: default
  pdf_document: default
  
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning = FALSE, message = FALSE)
library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
library(dplyr)
library(shinystan)
library(faux)
library(bayesplot)
library(tidyverse)
library(ggplot2)
library(gridExtra)
library(lubridate)
library(data.table)
library(Amelia)
library(tidyr)
library(MASS)
library(plyr)
library(ggplot2)
library(summarytools)
library(knitr)


options(scipen=999)

#setwd("C:/Users/mtrentman/IDrive-Sync/Postdoc/Estimating missing data/daily_predictions")
setwd("C:/Users/matt/IDrive-Sync/Postdoc/Estimating missing data/daily_predictions")
#setwd("~/GitHub/missing-data")
sp<-read.csv('daily.predictions.filled.csv',header = TRUE) ##Missing dates filled in
sp$date.f<-as.Date(sp$date.f,format="%Y-%m-%d")
sp$site_name<-as.character(sp$site_name)
#sp<-read.table(file = 'daily_predictions.tsv',header = TRUE) ##Raw data
sd<-read.table(file = 'site_data.tsv',header = TRUE)##Site data
sd$site_name<-as.character(sd$site_name)

##Function to switch font color depending on the output (HTML or PDF)
colorize <- function(x, color) {
  if (knitr::is_latex_output()) {
    sprintf("\\textcolor{%s}{%s}", color, x)
  } else if (knitr::is_html_output()) {
    sprintf("<span style='color: %s;'>%s</span>", color, 
      x)
  } else x
}

```
### Missing data in the context of Appling et al. dataset
I filled in the dates with missing data in the Appling et al. paper by site $\cdot$ year and made a historgram of missing data frequencies. I am making the assumption that if data were measured at the beginning and end of the year then they were likley collecting data for the whole year. There are obviously flaws to that assumption but it was the easiest way to address whether data was missing because it was actively being collected or not. The `r colorize("red line", "red")` is the percent of missing data for entire 10 year Shatto Ditch dataset. 

``````{r figs, echo=FALSE, fig.cap="Figure caption: The histogram shows the percent of missing data for each site x year with data measured on the first and last day of the year.", fig.show="hold", out.width="75%"}
# Find sites --------------------------------------------------------------

###Count NAs
cdata <- ddply(sp, c("site_name", "year"), summarize,
               N    = length(GPP),
               N.miss= sum(is.na(GPP)),
               Prop.miss=round(sum(is.na(GPP))/length(GPP)*100))


##Find sites with low missing data
full.year<-subset(cdata, N==365)

###Summary of NAs               
#Histogram
hist(full.year$Prop.miss, main="Distribution of percent missing data from Appling et al. site*years", xlab="Percent missing data")
abline(v=40.1, col="red")

#summary stats
summary(full.year$Prop.miss)
```


Next, I searched the Appling et al dataset for the site $\cdot$ year(s) with least amount of missing data. There was not a site $\cdot$ year combination with a full year of data, but there were two with less than 3% of data missing. 

```{r echo=FALSE, results='asis'}
##Find sites with low missing data
low.miss<-cdata[which(cdata$N==365 & cdata$Prop.miss<4),]


cols<-c(1:2,4:5)
knitr::kable(low.miss[,cols],col.names=(c("Site", "Year","Missing data (count)", "Prop. of year missing")),row.names = FALSE)
```

Fortunately, they each have somewhat unique annual GPP trends.



```{r figures-side, echo=FALSE, fig.cap="Figure caption: Two sites with low missing data from Appling et al. Data points represent the estimate and the error bars are the 95% credible interval. The line is modeled light using Yard et al. 1995 (NOTE: Light is scaled to fit on the graph) ", fig.show="hold", out.width="50%"}

low.miss.1<-subset(sp, site_name==low.miss$site_name[1] & year==2016)
low.miss.2<-subset(sp, site_name==low.miss$site_name[2] & year==2016)

##Simulate light for each site
hms<-rep("12:00:00", times=length(low.miss.1$date))
low.miss.1$date<-as.POSIXct(paste(low.miss.1$date.f, hms),tryFormats = c("%Y-%m-%d %H:%M:%OS"))
low.miss.2$date<-as.POSIXct(paste(low.miss.2$date.f, hms),tryFormats = c("%Y-%m-%d %H:%M:%OS"))

##light
# From Yard et al. (1995) Ecological Modelling.  Remember your trig?  
# calculate light as umol photon m-2 s-1.
# Arguments are:  
# time = a date and time input (posixct object)
# lat = latitude of field site
# longobs = longitude of field site
# longstd = standard longitude of the field site (NOTE: watch daylight savings time!!!). For PST, longstd is be 120 degrees. But during PDT it is 105 degrees. MST is 105 deg. MDT is 90. 


# convert degrees to radians
radi<-function(degrees){(degrees*pi/180)}

# function to estimate light
lightest<- function (time, lat, longobs, longstd) {
  jday<-yday(time)
  E<- 9.87*sin(radi((720*(jday-81))/365)) - 7.53*cos(radi((360*(jday-81))/365)) - 1.5*sin(radi((360*(jday-81))/365))
  LST<-as.numeric(time-trunc(time))
  ST<-LST+(3.989/1440)*(longstd-longobs)+E/1440
  solardel<- 23.439*sin(radi(360*((283+jday)/365)))
  hourangle<-(0.5-ST)*360
  theta<- acos( sin(radi(solardel)) * sin(radi(lat)) + cos(radi(solardel)) * cos(radi(lat)) * cos(radi(hourangle)) )
  suncos<-ifelse(cos(theta)<0, 0, cos(theta))
  GI<- suncos*2326
  GI	
  
}
low.miss.1$sim.light<-lightest(time=low.miss.1$date, lat=sd$lat[sd$site_name==low.miss$site_name[1]], longobs=sd$lon[sd$site_name==low.miss$site_name[1]],longstd=75)
low.miss.2$sim.light<-lightest(time=low.miss.2$date, lat=sd$lat[sd$site_name==low.miss$site_name[2]], longobs=sd$lon[sd$site_name==low.miss$site_name[2]],longstd=75) 

par(mar=c(4,4,.1,.1))
###Plot site data and light

main1<-sd$long_name[sd$site_name==low.miss$site_name[1]]
year1<-sd$long_name[sd$site_name==low.miss$year[1]]
main2<-sd$long_name[sd$site_name==low.miss$site_name[2]]
ggplot(data=low.miss.1, aes(x=date, y=GPP))+
  geom_line(aes(x=date, y=sim.light/700))+
  geom_point(color="black", size=3)+
  geom_errorbar(aes(x=date,ymin=GPP.lower, ymax=GPP.upper), width=0.2, size=0.5)+
  theme(legend.position="top")+
  theme_classic()+
  theme(axis.title.x=element_text(size=18,colour = "black"))+
  theme(axis.title.y=element_text(size=18,colour = "black"))+
  theme(axis.text.y=element_text(size=18,colour = "black"))+
  theme(axis.text.x=element_text(size=18,colour = "black"))+
  theme(legend.position="top")+
  ylab("GPP")+
  xlab("Time (days)")+
  ylim(c(0,7))+
  ggtitle(paste0(sd$long_name[sd$site_name==low.miss$site_name[1]],"-"," ", 2016))+
  annotate(geom="text", x=low.miss.1$date[300],
  y=6,label=paste(low.miss$Prop.miss[1],"% of data missing"),color="red")                 

ggplot(data=low.miss.2, aes(x=date, y=GPP))+
  geom_point(color="black", size=3)+
  geom_line(aes(x=date, y=sim.light/700))+
  geom_errorbar(aes(x=date,ymin=GPP.lower, ymax=GPP.upper), width=0.2, size=0.5)+
  theme(legend.position="top")+
  theme_classic()+
  theme(axis.title.x=element_text(size=18,colour = "black"))+
  theme(axis.title.y=element_text(size=18,colour = "black"))+
  theme(axis.text.y=element_text(size=18,colour = "black"))+
  theme(axis.text.x=element_text(size=18,colour = "black"))+
  theme(legend.position="top")+
  ylab("GPP")+
  xlab("Time (days)")+
  ylim(c(0,7))+
  ggtitle(paste0(sd$long_name[sd$site_name==low.miss$site_name[2]],"-"," ", 2016))+
  annotate(geom="text", x=low.miss.2$date[300],
  y=6,label=paste(low.miss$Prop.miss[2],"% of data missing"),color="red")  

```
### Estimating missing data on real datasets--Bayesian parameter estimation
I randomly removed data in 7-day blocks ranging from 2 to 60% of the original data.

```{r, echo=FALSE}
##Force some missing data-BY WEEK##
#Note-the model will still work if no data is missing but the objects in the next block of code (i.e., "y_index_mis", etc.)
#still need to be created
set.seed(50)
x<-low.miss.1$GPP
y_miss<-list(x,x,x,x,x,x)
#vector of missing number amounts
missing_n_tot<-c(0,25,50,100,150,300)

missing_n_week<-round(missing_n_tot/7)

#Create a list to store missing data integers
y_missing_integers<-c()

#Create initial missing data integer set. Then build on with the for loop
y_missing_integers[[1]]<-which(is.na(y_miss[[1]]))

#Adds the previous missing data integers to the next set. This makes them nested.
#i.e, all the missing data 10 integers are added to 15 more to make missing data 15.
for(i in 2:length(missing_n_week)){
  h<-which(x %in% sample(x,(missing_n_week[i]-missing_n_week[i-1]), replace = FALSE))
  y_missing_integers[[i]]<-c(h,y_missing_integers[[i-1]])
}


for(i in 2:length(missing_n_week)){
  q<-y_missing_integers[[i]]-3
  w<-y_missing_integers[[i]]-2
  e<-y_missing_integers[[i]]-1
  t<-y_missing_integers[[i]]+1
  y<-y_missing_integers[[i]]+2
  u<-y_missing_integers[[i]]+3
  y_missing_integers[[i]]<-c(q,w,e,t,y,u)
}

##Remove last value from indexes 6-8. Unique to this random seed and weeks.
#Accidentally added a missing value to the end because it picked a value close to the end



#Assign NAs to the missing data in each list
for(i in 1:length(y_missing_integers)){
  z<-y_miss[[i]]
  z[y_missing_integers[[i]]]<-NA
  #z[1]<-x[1]
  y_miss[[i]]<-z
}

y_miss[[6]]<-y_miss[[6]][1:length(y_miss[[6]])-1]

miss<-sapply(y_miss, function(x) sum(length(which(is.na(x)))))
prop.miss<-round(miss/length(x)*100)
prop.miss

##Create objects with the location (index) of missing or observed data AND number
## of missing or observed data
#FOR SINGLE VECTOR
#y_index_mis <- which(is.na(y_miss)) # Identify the rows for each variable in airquality with NA
#y_index_obs <- which(!is.na(y_miss)) # Identify the rows for each variable in airquality with observed data
#y_nMiss <- length(y_index_mis)# How many NAs?
#y_nObs <- length(y_index_obs) # How many NAs?

##Create objects with the location (index) of missing or observed data AND number
## of missing or observed data
#FOR LIST OF VECTORS
y_index_mis <- lapply(y_miss,function(var){which(is.na(var))}) # Identify the rows for each variable in airquality with NA
y_index_obs <- lapply(y_miss,function(var){which(!is.na(var))}) # Identify the rows for each variable in airquality with observed data
y_nMiss <- lapply(y_index_mis,function(var){length(var)}) # How many NAs?
y_nObs <- lapply(y_index_obs,function(var){length(var)}) # How many NAs?


##Replace NAs with arbitrary number to make Stan happy
for(i in 1:length(missing_n_week)){
  r<-y_miss[[i]]
  r[y_missing_integers[[i]]]<--100 #arbitrary number
  r[1]<-x[1]
  y_miss[[i]]<-r
} 
y_miss[[6]]<-y_miss[[6]][1:length(y_miss[[6]])-1]

```
Then used an AR(1) missing data process-model with an intercept and light covariate to model the parameters and missing data.


```{r,eval=FALSE}
"
 
/*----------------------- Data --------------------------*/
  /* Data block: defines the objects that will be inputted as data */
  data {
    int N; // Length of state and observation time series
    vector[N] y_miss; // Observations
    real z0; // Initial state value
    vector[N] light; //log of light observations
    int y_nMiss; // number of missing values
    int y_index_mis[y_nMiss]; // index or location of missing values within the dataset
  }
/*----------------------- Parameters --------------------------*/
  /* Parameter block: defines the variables that will be sampled */
  parameters {
    vector<lower = 0>[y_nMiss] y_imp;// Missing data
    real<lower=0> sdp; // Standard deviation of the process equation
    //real<lower=0> sdo; // Standard deviation of the observation equation
    real b0;
    real b1;
    real<lower = 0, upper=1 > phi; // Auto-regressive parameter
    //vector[N] z; // State time series
  }
  transformed parameters { 
    vector[N] y;
    y=y_miss; // makes the data a transformed variable
    y[y_index_mis] =y_imp; // replaces missing data in y with estimated data
    } 
  /*----------------------- Model --------------------------*/
  /* Model block: defines the model */
  model {
    // Prior distributions
    sdp ~ normal(0, 1);
    phi ~ beta(1,1);
    b0 ~ normal(0,5);
    b1 ~ normal(0,5);
    
    // Distribution for the first value
    y[1] ~ normal(z0, sdp);
   
    // Distributions for all other states
    for(t in 2:N){
      y[t] ~ normal(b0+y[t-1]*phi+light[t]*b1, sdp);
    }
    
   generated quantities {
    vector[N] y_rep; // replications from posterior predictive dist
    y_rep[1]=normal_rng(y[1], 0.1);
 
    for (t in 2:N) {
    y_rep[t]=normal_rng(b0+y[t-1]*phi+light[t]*b1, sdp);
 }
  }
"
```

```{r, echo=FALSE,out.width="49%",out.height="20%",fig.cap="caption",fig.show='hold',fig.align='center'}

knitr::include_graphics(c("/Users/matt/Documents/GitHub/missing-data/lowmiss1_bayes_param.jpeg","/Users/matt/Documents/GitHub/missing-data/lowmiss2_ts_60perc.jpeg"))
``` 

```{r, echo=FALSE,out.width="49%",out.height="20%",fig.cap="caption",fig.show='hold',fig.align='center'}

knitr::include_graphics(c("/Users/matt/Documents/GitHub/missing-data/lowmiss1_ts_60perc.jpg","/Users/matt/Documents/GitHub/missing-data/lowmiss2_ts_60perc.jpeg"))
``` 